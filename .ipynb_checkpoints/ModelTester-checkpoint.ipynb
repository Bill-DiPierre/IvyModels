{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dipierre/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#for displaying result\n",
    "import csv\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "#these two lines changes jupyter's variable display to put each variable on its own line\n",
    "#that way, we can dump mulitple variables from a single code cell (without them overwriting the previous)\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAllDataFiles():\n",
    "    filenames=[]\n",
    "    filenames.append(\"./data/FinAid_Labeled.csv\")\n",
    "    dumpColumnTitles()\n",
    "    for file in filenames:\n",
    "        X_train, X_test, y_train, y_test = loadOneFile(file)\n",
    "        runAllVectorizers(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def loadOneFile(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    X = df.question\n",
    "    y = df.Intent_Number\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dumpColumnTitles():\n",
    "    f.write(\"Stop Words, nGram Range, Max Doc Frequency, Min Doc Frequency, \")\n",
    "    for model in models:\n",
    "        f.write(model['name'] + ', ')\n",
    "    f.write ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAllVectorizers(X_train, X_test, y_train, y_test):\n",
    "    i = 0\n",
    "    for vect in vectorizers:\n",
    "        i += 1\n",
    "        print (\"Running Vectorizer {} of {}\".format(i, len(vectorizers)), end=\"\\r\")\n",
    "        X_train_dtm, X_test_dtm = runOneVectorizer(vect,X_train, X_test, y_train, y_test)\n",
    "        runAllModels(X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "        f.write('\\n')\n",
    "    print(\"                                                         \", end=\"\\r\")\n",
    "    \n",
    "\n",
    "def runOneVectorizer(vect,X_train, X_test, y_train, y_test):\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "    if vect.stop_words is None:\n",
    "        f.write(\"None\")\n",
    "    else:\n",
    "        f.write(str(vect.stop_words))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.ngram_range).replace(',','-'))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.max_df))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.min_df))\n",
    "    f.write(',')\n",
    "    \n",
    "    return X_train_dtm, X_test_dtm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAllModels(X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    for model in models:\n",
    "        runOneModel(model['model'],X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "\n",
    "\n",
    "def runOneModel(model,X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    # train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "    #%time model.fit(X_train_dtm, y_train)\n",
    "    model.fit(X_train_dtm, y_train)\n",
    "\n",
    "    # make class predictions for X_test_dtm\n",
    "    y_pred_class = model.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "    f.write(str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "    f.write(',')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vectorizers & Models, then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#tweak the vectorizing settings here:\n",
    "vectorizers=[]\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 1), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 1), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 2), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 1), min_df=2, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 1), min_df=2, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 2), min_df=2, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 1), min_df=1, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 1), min_df=1, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 2), min_df=1, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=1, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 1), min_df=2, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 1), min_df=2, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words= None,     ngram_range=(1, 2), min_df=2, max_df= 30))\n",
    "vectorizers.append(CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2, max_df= 30))\n",
    "\n",
    "models = []\n",
    "models.append({'model':MultinomialNB(), 'name': 'Naive Bayes'})\n",
    "models.append({'model':LogisticRegression(), 'name': 'Logistic Regression'})\n",
    "models.append({'model':svm.SVC(), 'name': 'Linear SVC'})\n",
    "models.append({'model':RandomForestClassifier(n_estimators = 50), 'name': 'Random Forest'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Stop Words</td><td>nGram Range</td><td>Max Doc Frequency</td><td>Min Doc Frequency</td><td>Naive Bayes   </td><td>Logistic Regression</td><td>Linear SVC    </td><td>Random Forest </td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 1)     </td><td>1.0              </td><td>1                </td><td>0.274193548387</td><td>0.39247311828      </td><td>0.161290322581</td><td>0.440860215054</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 1)     </td><td>1.0              </td><td>1                </td><td>0.306451612903</td><td>0.376344086022     </td><td>0.161290322581</td><td>0.435483870968</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 2)     </td><td>1.0              </td><td>1                </td><td>0.306451612903</td><td>0.387096774194     </td><td>0.161290322581</td><td>0.403225806452</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 2)     </td><td>1.0              </td><td>1                </td><td>0.301075268817</td><td>0.376344086022     </td><td>0.161290322581</td><td>0.397849462366</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 1)     </td><td>1.0              </td><td>2                </td><td>0.290322580645</td><td>0.39247311828      </td><td>0.215053763441</td><td>0.408602150538</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 1)     </td><td>1.0              </td><td>2                </td><td>0.322580645161</td><td>0.365591397849     </td><td>0.225806451613</td><td>0.403225806452</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 2)     </td><td>1.0              </td><td>2                </td><td>0.301075268817</td><td>0.39247311828      </td><td>0.177419354839</td><td>0.39247311828 </td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 2)     </td><td>1.0              </td><td>2                </td><td>0.322580645161</td><td>0.354838709677     </td><td>0.220430107527</td><td>0.39247311828 </td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 1)     </td><td>30               </td><td>1                </td><td>0.290322580645</td><td>0.338709677419     </td><td>0.161290322581</td><td>0.344086021505</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 1)     </td><td>30               </td><td>1                </td><td>0.290322580645</td><td>0.311827956989     </td><td>0.161290322581</td><td>0.376344086022</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 2)     </td><td>30               </td><td>1                </td><td>0.327956989247</td><td>0.354838709677     </td><td>0.161290322581</td><td>0.344086021505</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 2)     </td><td>30               </td><td>1                </td><td>0.317204301075</td><td>0.349462365591     </td><td>0.161290322581</td><td>0.403225806452</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 1)     </td><td>30               </td><td>2                </td><td>0.311827956989</td><td>0.333333333333     </td><td>0.161290322581</td><td>0.344086021505</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 1)     </td><td>30               </td><td>2                </td><td>0.306451612903</td><td>0.311827956989     </td><td>0.161290322581</td><td>0.360215053763</td><td></td></tr>\n",
       "<tr><td>None      </td><td>(1- 2)     </td><td>30               </td><td>2                </td><td>0.301075268817</td><td>0.333333333333     </td><td>0.161290322581</td><td>0.311827956989</td><td></td></tr>\n",
       "<tr><td>english   </td><td>(1- 2)     </td><td>30               </td><td>2                </td><td>0.290322580645</td><td>0.344086021505     </td><td>0.161290322581</td><td>0.354838709677</td><td></td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename='output.csv'\n",
    "f = open(filename, 'w')\n",
    "loadAllDataFiles()\n",
    "f.close()\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "\n",
    "display(HTML(tabulate.tabulate(data, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
