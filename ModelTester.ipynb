{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dipierre/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#for displaying result\n",
    "import csv\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "#these two lines changes jupyter's variable display to put each variable on its own line\n",
    "#that way, we can dump mulitple variables from a single code cell (without them overwriting the previous)\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAllDataFiles():\n",
    "    filenames=[]\n",
    "#    filenames.append(\"./data/FinAid_Labeled.csv\")\n",
    "    filenames.append(\"./data/Career_Labeled.csv\")\n",
    "    dumpColumnTitles()\n",
    "    for file in filenames:\n",
    "        X_train, X_test, y_train, y_test = loadOneFile(file)\n",
    "        runAllVectorizers(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def loadOneFile(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    X = df.question\n",
    "    y = df.Intent_Number\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dumpColumnTitles():\n",
    "    f.write(\"Tokenizer, Stop Words, nGram Range, Max Doc Frequency, Min Doc Frequency, \")\n",
    "    for model in models:\n",
    "        f.write(model['name'] + ', ')\n",
    "    f.write ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAllVectorizers(X_train, X_test, y_train, y_test):\n",
    "    i = 0\n",
    "    for vect in vectorizers:\n",
    "        i += 1\n",
    "        print (\"Running Vectorizer {} of {}\".format(i, len(vectorizers)), end=\"\\r\")\n",
    "        X_train_dtm, X_test_dtm = runOneVectorizer(vect,X_train, X_test, y_train, y_test)\n",
    "        runAllModels(X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "        f.write('\\n')\n",
    "    print(\"                                                         \", end=\"\\r\")\n",
    "    \n",
    "\n",
    "def runOneVectorizer(vect,X_train, X_test, y_train, y_test):\n",
    "    # learn training data vocabulary, then use it to create a document-term matrix\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "    # transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "    if vect.tokenizer is None:\n",
    "        f.write(\"None\")\n",
    "    else:\n",
    "        name=str(vect.tokenizer)\n",
    "        name=name[10:name.find('T')]\n",
    "        f.write(name)\n",
    "    f.write(',')\n",
    "    if vect.stop_words is None:\n",
    "        f.write(\"None\")\n",
    "    elif type(vect.stop_words) is frozenset:\n",
    "        f.write(\"Custom\")\n",
    "    else:\n",
    "        f.write(str(vect.stop_words))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.ngram_range).replace(',','-'))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.max_df))\n",
    "    f.write(',')\n",
    "    f.write(str(vect.min_df))\n",
    "    f.write(',')\n",
    "    \n",
    "    return X_train_dtm, X_test_dtm\n",
    "\n",
    "\n",
    "def duplicateVectorizer(vect):\n",
    "    #create a new vectorizer that is dupe of current one in the array\n",
    "    newVect = CountVectorizer()\n",
    "    newVect.stop_words = vect.stop_words\n",
    "    newVect.tokenizer = vect.tokenizer\n",
    "    newVect.ngram_range = vect.ngram_range\n",
    "    newVect.min_df = vect.min_df\n",
    "    newVect.max_df = vect.max_df\n",
    "    return newVect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAllModels(X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    for model in models:\n",
    "        runOneModel(model['model'],X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "\n",
    "\n",
    "def runOneModel(model,X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    # train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "    #%time model.fit(X_train_dtm, y_train)\n",
    "    model.fit(X_train_dtm, y_train)\n",
    "\n",
    "    # make class predictions for X_test_dtm\n",
    "    y_pred_class = model.predict(X_test_dtm)\n",
    "\n",
    "    # calculate accuracy of class predictions\n",
    "    f.write(str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "    f.write(',')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "from string import punctuation\n",
    "\n",
    "def customStopWords():\n",
    "    #additional_stop_words = frozenset(['testthisextrastopword'])\n",
    "    additional_stop_words = set(list(punctuation))\n",
    "    return text.ENGLISH_STOP_WORDS.union(additional_stop_words)  #add to 'english' list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text, stemmer):\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])  #strip out punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "def PorterTokenizer(text):\n",
    "    return tokenize(text, PorterStemmer())\n",
    "\n",
    "def LancasterTokenizer(text):\n",
    "    return tokenize(text, LancasterStemmer())\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vectorizers & Models, then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodels.append({'model':LogisticRegression(), 'name': 'Logistic Regression'})\\nmodels.append({'model':svm.SVC(), 'name': 'Linear SVC'})\\nmodels.append({'model':RandomForestClassifier(n_estimators = 50), 'name': 'Random Forest'})\\nmodels.append({'model':KNeighborsClassifier(), 'name': 'K Neighbors'})\\nmodels.append({'model':DecisionTreeClassifier(), 'name': 'Decision Tree'})\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "#tweak the vectorizing settings here:\n",
    "stopWords = customStopWords()\n",
    "vectorizers=[]\n",
    "vectorizers.append(CountVectorizer(tokenizer= LancasterTokenizer, stop_words= None, ngram_range=(1, 1), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(tokenizer= LancasterTokenizer, stop_words= None, ngram_range=(1, 2), min_df=1, max_df=1.0))\n",
    "vectorizers.append(CountVectorizer(tokenizer= LancasterTokenizer, stop_words= None, ngram_range=(1, 3), min_df=1, max_df=1.0))\n",
    "\n",
    "#each of these loops will add a full set of existing vectorizers with a single property changed\n",
    "#each loop doubles the number of vectorizers\n",
    "#to have more than two settings for a single feature, best to add manually to the inital append statements above (otherwise you will get duplicate entries)\n",
    "    \n",
    "\"\"\" leave out for now, have three values for this above    \n",
    "count = len(vectorizers)\n",
    "for i in range(count):\n",
    "    newVect = duplicateVectorizer(vectorizers[i])\n",
    "    newVect.ngram_range =(1,2)\n",
    "    vectorizers.append(newVect)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "count = len(vectorizers)\n",
    "for i in range(count):\n",
    "    newVect = duplicateVectorizer(vectorizers[i])\n",
    "    newVect.stop_words='english'\n",
    "    vectorizers.append(newVect)\n",
    "\n",
    "count = len(vectorizers)\n",
    "for i in range(count):\n",
    "    newVect = duplicateVectorizer(vectorizers[i])\n",
    "    newVect.min_df = 2\n",
    "    vectorizers.append(newVect)\n",
    "\n",
    "count = len(vectorizers)\n",
    "for i in range(count):\n",
    "    newVect = duplicateVectorizer(vectorizers[i])\n",
    "    newVect.tokenizer = PorterTokenizer\n",
    "    vectorizers.append(newVect)\n",
    "\n",
    "count = len(vectorizers)\n",
    "for i in range(count):\n",
    "    newVect = duplicateVectorizer(vectorizers[i])\n",
    "    newVect.max_df = 100\n",
    "    vectorizers.append(newVect)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append({'model':MultinomialNB(), 'name': 'Naive Bayes'})\n",
    "\"\"\"\n",
    "models.append({'model':LogisticRegression(), 'name': 'Logistic Regression'})\n",
    "models.append({'model':svm.SVC(), 'name': 'Linear SVC'})\n",
    "models.append({'model':RandomForestClassifier(n_estimators = 50), 'name': 'Random Forest'})\n",
    "models.append({'model':KNeighborsClassifier(), 'name': 'K Neighbors'})\n",
    "models.append({'model':DecisionTreeClassifier(), 'name': 'Decision Tree'})\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Tokenizer</td><td>Stop Words</td><td>nGram Range</td><td>Max Doc Frequency</td><td>Min Doc Frequency</td><td>Naive Bayes   </td><td></td></tr>\n",
       "<tr><td>Lancaster</td><td>None      </td><td>(1- 1)     </td><td>1.0              </td><td>1                </td><td>0.446611909651</td><td></td></tr>\n",
       "<tr><td>Lancaster</td><td>None      </td><td>(1- 2)     </td><td>1.0              </td><td>1                </td><td>0.448665297741</td><td></td></tr>\n",
       "<tr><td>Lancaster</td><td>None      </td><td>(1- 3)     </td><td>1.0              </td><td>1                </td><td>0.433264887064</td><td></td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename='./output/output.csv'\n",
    "f = open(filename, 'w')\n",
    "loadAllDataFiles()\n",
    "f.close()\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "display(HTML(tabulate.tabulate(data, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpData():\n",
    "    filename= \"./data/FinAid_Labeled.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    stopWords = customStopWords()\n",
    "    vect = CountVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0)\n",
    "    #vect.stop_words = stopWords\n",
    "    #vect.analyzer='word'\n",
    "\n",
    "    X = df.question\n",
    "    y = df.Intent_Number\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print(vect.get_feature_names())\n",
    "    #print(vect.token_pattern)\n",
    "    #X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    #X_train_dtm2 = vect.fit_transform(X_train)\n",
    " #   print(vect.get_feature_names())\n",
    "    #print(vect.token_pattern)\n",
    "\n",
    "    #vect.tokenizer = LancasterTokenizer()\n",
    "    #X_train_dtm3 = vect.fit_transform(X_train)\n",
    "    #print(vect.get_feature_names())\n",
    "    #print(vect.vocabulary)\n",
    "    #print(vect.token_pattern)\n",
    "    #X_test_dtm2 = vect.transform(X_test)\n",
    "\n",
    "    #pd.DataFrame( X_train_dtm.todense(),columns=vect.get_feature_names())\n",
    "    #print(vect.get_feature_names())\n",
    "    #print(vect.tokenizer)\n",
    "    #print(vect.stop_words)\n",
    "    \n",
    "#dumpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab = ['The swimmer likes swimming so he swims. 1. ... \"\" 33']\n",
    "vect = CountVectorizer().fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform(['The swimmer likes swimming.'])\n",
    "sentence2 = vec.transform(['The swimmer swims.'])\n",
    "\n",
    "print('Vocabulary: %s' %vec.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())\n",
    "\n",
    "\n",
    "def xtokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "######## \n",
    "\n",
    "\n",
    "def xxxtokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "#vect = CountVectorizer(tokenizer=tokenize, stop_words='english') \n",
    "\n",
    "vect.fit(vocab)\n",
    "\n",
    "sentence1 = vect.transform(['The swimmer likes swimming. \"\" 1. 2 33'])\n",
    "sentence2 = vect.transform(['The swimmer swims.'])\n",
    "\n",
    "print('Vocabulary: %s' %vect.get_feature_names())\n",
    "print('Sentence 1: %s' %sentence1.toarray())\n",
    "print('Sentence 2: %s' %sentence2.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
